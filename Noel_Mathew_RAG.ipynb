{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import pytesseract\n",
    "from pdfminer.high_level import extract_text\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import cv2  # For multilingual OCR\n",
    "import os\n",
    "import numpy as np\n",
    "import fitz\n",
    "import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(r\"D:\\SEM 5\\RAG\\sample_pdfs\\en\\Blue_Ocean_Strategy,_Expanded_Edition_How_to_Create_Uncontested-2.pdf\") \n",
    "out = open(\"output.txt\", \"wb\")\n",
    "for page in doc: \n",
    "    text = page.get_text().encode(\"utf8\") \n",
    "    out.write(text) \n",
    "    out.write(bytes((12,)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path, language = 'eng'):\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            text += page_text\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the PDFs\n",
    "\n",
    "def process_pdf_folder(input_folder, output_folder, language = 'eng'):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(input_folder, filename)\n",
    "            print(f\"Found PDF: {pdf_path}...\")\n",
    "\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            if text is None:\n",
    "                print(f'No text found in {filename}, Applying OCR using pytesseract...')\n",
    "                text = ocr_from_pdf(pdf_path, language)\n",
    "\n",
    "            output_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "            with open(output_path, 'w', encoding='utf-8') as file_out:\n",
    "                file_out.write(text)\n",
    "\n",
    "            print(f\"Saved text to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\en\\Blue_Ocean_Strategy,_Expanded_Edition_How_to_Create_Uncontested-2.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\en\\Blue_Ocean_Strategy,_Expanded_Edition_How_to_Create_Uncontested-2.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\en\\Reboot_Leadership_and_the_Art_of.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\en\\Reboot_Leadership_and_the_Art_of.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\en\\The Alchemist by Paulo Coelho-1.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\en\\The Alchemist by Paulo Coelho-1.txt\n"
     ]
    }
   ],
   "source": [
    "# English PDFs\n",
    "\n",
    "eng_pdfs= r\"D:\\SEM 5\\RAG\\sample_pdfs\\en\"\n",
    "output_folder_en = r\"D:\\SEM 5\\RAG\\converted_files\\en\"\n",
    "\n",
    "process_pdf_folder(eng_pdfs, output_folder_en, language = 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\15092024_142.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\15092024_142.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\471 (TO).pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\471 (TO).txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\AP Ramjan.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\AP Ramjan.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\NEC-14.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\NEC-14.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\Research Nirdeshika.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\Research Nirdeshika.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\আহম্মেদNOC.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\আহম্মেদNOC.txt\n"
     ]
    }
   ],
   "source": [
    "# Bengali PDFs\n",
    "\n",
    "bengali_pdfs= r\"D:\\SEM 5\\RAG\\sample_pdfs\\bn\"\n",
    "output_folder_bn = r\"D:\\SEM 5\\RAG\\converted_files\\bn\"\n",
    "\n",
    "process_pdf_folder(bengali_pdfs, output_folder_bn, language = 'ben')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\12-Rabiul-Awal-2024.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\12-Rabiul-Awal-2024.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\Extension-of-Ahdoc-Employees.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\Extension-of-Ahdoc-Employees.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\fasana-e-ajaib final.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\fasana-e-ajaib final.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\Notification-for-Other-Nationals.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\Notification-for-Other-Nationals.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\shora e rampur.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\shora e rampur.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\Solidarity-Day.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\Solidarity-Day.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\حیات جاوید، سوانح سر سید احمد خاں.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\حیات جاوید، سوانح سر سید احمد خاں.txt\n"
     ]
    }
   ],
   "source": [
    "# Urdu PDFs\n",
    "\n",
    "urdu_pdfs= r\"D:\\SEM 5\\RAG\\sample_pdfs\\ur\"\n",
    "output_folder_ur = r\"D:\\SEM 5\\RAG\\converted_files\\ur\"\n",
    "\n",
    "process_pdf_folder(urdu_pdfs, output_folder_ur, language = 'urd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\1553a07b-9f53-4e8b-9987-ae714000b95b.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\1553a07b-9f53-4e8b-9987-ae714000b95b.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\98aab034-f8d7-4f6e-9a0c-b52c12f55ce7.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\98aab034-f8d7-4f6e-9a0c-b52c12f55ce7.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\ec2def2f-cc7b-44f3-87d1-24dc82f3a0ca.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\ec2def2f-cc7b-44f3-87d1-24dc82f3a0ca.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\P020230313555181904759.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\P020230313555181904759.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\P020230907694757200665.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\P020230907694757200665.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\P020230907695746624812.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\P020230907695746624812.txt\n"
     ]
    }
   ],
   "source": [
    "# Chinese PDFs\n",
    "\n",
    "chinese_pdfs= r\"D:\\SEM 5\\RAG\\sample_pdfs\\zh\"\n",
    "output_folder_zh = r\"D:\\SEM 5\\RAG\\converted_files\\zh\"\n",
    "\n",
    "process_pdf_folder(chinese_pdfs, output_folder_zh, language = 'chi_sim')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text_file_folder = r\"D:\\SEM 5\\RAG\\converted_files\"\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Iterate through text files and generate embeddings\n",
    "text_data = []\n",
    "file_names = []\n",
    "\n",
    "for root, dirs, files in os.walk(text_file_folder):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.txt'):  # Check for text files\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            text = read_text_file(file_path)\n",
    "            text_data.append(text)  # Store the text content\n",
    "            file_names.append(file_name)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 384)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(text_data, convert_to_numpy= True)\n",
    "print(embeddings.shape)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings = embeddings/np.linalg.norm(embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 22 text files with 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"Indexed {len(embeddings)} text files with {dim} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'faiss_index.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "import pinecone\n",
    "pinecone.init\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index('faiss_index.bin')\n",
    "\n",
    "# Load the text files\n",
    "text_files = [os.path.join(text_file_folder, file_name) for file_name in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noelm\\AppData\\Local\\Temp\\ipykernel_3784\\3409896792.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'pdf-rag-system' created and connected.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "PINECONE_API_KEY = os.getenv('PINE_API')\n",
    " \n",
    "index_name=\"pdf-rag-system\"\n",
    "\n",
    "# Initialize Pinecone using the Pinecone class\n",
    "pc = Pinecone(\n",
    "    api_key='7cfea3c1-2d79-4c22-8ec7-9e5fb82897e3'\n",
    ")\n",
    "\n",
    "# Check if the index already exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # Create an index with the desired name and dimension\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384 ,  # Adjust dimension based on your embeddings\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws',  # Specify cloud provider, e.g., 'aws'\n",
    "            region='us-east-1'  # Try using a region like 'us-east-1'\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Connect to the created index\n",
    "index = pc.Index(index_name)\n",
    "print(f\"Index '{index_name}' created and connected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'pdf-rag-system' connected.\n"
     ]
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "print(f\"Index '{index_name}' connected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# Assume embeddings and index are initialized\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "# Generate embedding for the query\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Perform similarity search on Pinecone\n",
    "search_results = index.query(\n",
    "    namespace=\"ns1\",  # Replace with your actual namespace\n",
    "    vector=query_embedding,\n",
    "    top_k=5,  # Get more results for training\n",
    "    include_values=True,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "# Extract text from the search results\n",
    "retrieved_texts = []\n",
    "for match in search_results['matches']:\n",
    "    retrieved_texts.append(match['metadata']['text'])  # Assuming metadata has the text field\n",
    "\n",
    "# Combine retrieved documents into one dataset\n",
    "training_data = \"\\n\".join(retrieved_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training data to a text file (simple format for fine-tuning)\n",
    "with open('training_data.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "# Load the training data\n",
    "def load_dataset(file_path, tokenizer):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=128  # Tokenize into blocks\n",
    "    )\n",
    "\n",
    "train_dataset = load_dataset('training_data.txt', tokenizer)\n",
    "\n",
    "# Data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Masked language modeling (if applicable, change to True)\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",  # Save the fine-tuned model here\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,  # Adjust based on your needs\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      " The response is a continuation of the story, where the alchemist is guiding the boy to the Pyramids, teaching him about the Language of the World, and showing him the process of transforming lead into gold. The alchemist also talks about the Philosopher's Stone, the Elixir of Life, and the Master Work. \n",
      "\n",
      "The story then shifts to the Englishman, who is studying alchemy and trying to find the Philosopher's Stone. He meets an Arabian alchemist who is said to have discovered the Philosopher's Stone and the Elixir of Life. The Englishman joins the alchemist on a journey through the desert, where they encounter various obstacles and the alchemist teaches the Englishman about the language of omens.\n",
      "\n",
      "As they near the end of their journey, the alchemist reveals that the boy has learned everything he needs to know through his actions and experiences, and only needs to learn one more thing. The story then cuts to a scene where the alchemist and the boy are stopped by a group of tribal men, who accuse them of being spies. The alchemist claims that they are just travelers and offers to show them the boy's extraordinary powers, which leads to a tense confrontation.\n",
      "\n",
      "The story raises questions about the nature of alchemy, the power of transformation, and the relationship between the alchemist and the boy. It also explores the themes of nature, destiny, and the language of omens.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from groq import Groq\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Initialize Groq client with your API key\n",
    "client = Groq(\n",
    "    api_key='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',  # Replace with your actual Groq API key\n",
    ")\n",
    "\n",
    "# Load your training data (you may want to preprocess this for your needs)\n",
    "def load_training_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "training_data = load_training_data('training_data.txt')\n",
    "\n",
    "# Prepare a prompt for the model\n",
    "prompt = f\"Using the following training data, generate a response:\\n{training_data}\\n\\nResponse:\"\n",
    "\n",
    "# Create a chat completion request\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-70b-8192\",  # Specify the model you want to use\n",
    ")\n",
    "\n",
    "# Extract and print the generated response\n",
    "response_content = chat_completion.choices[0].message.content\n",
    "print(\"Generated Response:\\n\", response_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
