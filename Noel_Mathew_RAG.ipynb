{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import pytesseract\n",
    "from pdfminer.high_level import extract_text\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import cv2  # For multilingual OCR\n",
    "import os\n",
    "import numpy as np\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(r\"D:\\SEM 5\\RAG\\sample_pdfs\\en\\Blue_Ocean_Strategy,_Expanded_Edition_How_to_Create_Uncontested-2.pdf\") \n",
    "out = open(\"output.txt\", \"wb\")\n",
    "for page in doc: \n",
    "    text = page.get_text().encode(\"utf8\") \n",
    "    out.write(text) \n",
    "    out.write(bytes((12,)))\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path, language = 'eng'):\n",
    "    try:\n",
    "        doc = pymupdf.open(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            page_text = page.get_text()\n",
    "            text += page_text\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the PDFs\n",
    "\n",
    "def process_pdf_folder(input_folder, output_folder, language = 'eng'):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(input_folder, filename)\n",
    "            print(f\"Found PDF: {pdf_path}...\")\n",
    "\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            if text is None:\n",
    "                print(f'No text found in {filename}, Applying OCR using pytesseract...')\n",
    "                text = ocr_from_pdf(pdf_path, language)\n",
    "\n",
    "            output_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "            with open(output_path, 'w', encoding='utf-8') as file_out:\n",
    "                file_out.write(text)\n",
    "\n",
    "            print(f\"Saved text to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\en\\Blue_Ocean_Strategy,_Expanded_Edition_How_to_Create_Uncontested-2.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\en\\Blue_Ocean_Strategy,_Expanded_Edition_How_to_Create_Uncontested-2.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\en\\Reboot_Leadership_and_the_Art_of.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\en\\Reboot_Leadership_and_the_Art_of.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\en\\The Alchemist by Paulo Coelho-1.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\en\\The Alchemist by Paulo Coelho-1.txt\n"
     ]
    }
   ],
   "source": [
    "# English PDFs\n",
    "\n",
    "eng_pdfs= r\"D:\\SEM 5\\RAG\\sample_pdfs\\en\"\n",
    "output_folder_en = r\"D:\\SEM 5\\RAG\\converted_files\\en\"\n",
    "\n",
    "process_pdf_folder(eng_pdfs, output_folder_en, language = 'eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\15092024_142.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\15092024_142.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\471 (TO).pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\471 (TO).txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\AP Ramjan.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\AP Ramjan.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\NEC-14.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\NEC-14.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\Research Nirdeshika.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\Research Nirdeshika.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\bn\\আহম্মেদNOC.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\bn\\আহম্মেদNOC.txt\n"
     ]
    }
   ],
   "source": [
    "# Bengali PDFs\n",
    "\n",
    "bengali_pdfs= r\"D:\\SEM 5\\RAG\\sample_pdfs\\bn\"\n",
    "output_folder_bn = r\"D:\\SEM 5\\RAG\\converted_files\\bn\"\n",
    "\n",
    "process_pdf_folder(bengali_pdfs, output_folder_bn, language = 'ben')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\12-Rabiul-Awal-2024.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\12-Rabiul-Awal-2024.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\Extension-of-Ahdoc-Employees.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\Extension-of-Ahdoc-Employees.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\fasana-e-ajaib final.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\fasana-e-ajaib final.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\Notification-for-Other-Nationals.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\Notification-for-Other-Nationals.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\shora e rampur.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\shora e rampur.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\Solidarity-Day.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\Solidarity-Day.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\ur\\حیات جاوید، سوانح سر سید احمد خاں.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\ur\\حیات جاوید، سوانح سر سید احمد خاں.txt\n"
     ]
    }
   ],
   "source": [
    "# Urdu PDFs\n",
    "\n",
    "urdu_pdfs= r\"D:\\SEM 5\\RAG\\sample_pdfs\\ur\"\n",
    "output_folder_ur = r\"D:\\SEM 5\\RAG\\converted_files\\ur\"\n",
    "\n",
    "process_pdf_folder(urdu_pdfs, output_folder_ur, language = 'urd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\1553a07b-9f53-4e8b-9987-ae714000b95b.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\1553a07b-9f53-4e8b-9987-ae714000b95b.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\98aab034-f8d7-4f6e-9a0c-b52c12f55ce7.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\98aab034-f8d7-4f6e-9a0c-b52c12f55ce7.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\ec2def2f-cc7b-44f3-87d1-24dc82f3a0ca.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\ec2def2f-cc7b-44f3-87d1-24dc82f3a0ca.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\P020230313555181904759.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\P020230313555181904759.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\P020230907694757200665.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\P020230907694757200665.txt\n",
      "Found PDF: D:\\SEM 5\\RAG\\sample_pdfs\\zh\\P020230907695746624812.pdf...\n",
      "Saved text to D:\\SEM 5\\RAG\\converted_files\\zh\\P020230907695746624812.txt\n"
     ]
    }
   ],
   "source": [
    "# Chinese PDFs\n",
    "\n",
    "chinese_pdfs= r\"D:\\SEM 5\\RAG\\sample_pdfs\\zh\"\n",
    "output_folder_zh = r\"D:\\SEM 5\\RAG\\converted_files\\zh\"\n",
    "\n",
    "process_pdf_folder(chinese_pdfs, output_folder_zh, language = 'chi_sim')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text_file_folder = r\"D:\\SEM 5\\RAG\\converted_files\"\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Iterate through text files and generate embeddings\n",
    "text_data = []\n",
    "file_names = []\n",
    "\n",
    "for root, dirs, files in os.walk(text_file_folder):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.txt'):  # Check for text files\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            text = read_text_file(file_path)\n",
    "            text_data.append(text)  # Store the text content\n",
    "            file_names.append(file_name)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 384)\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(text_data, convert_to_numpy= True)\n",
    "print(embeddings.shape)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "embeddings = embeddings/np.linalg.norm(embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 22 text files with 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(f\"Indexed {len(embeddings)} text files with {dim} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'faiss_index.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'documents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example query\u001b[39;00m\n\u001b[0;32m     14\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFind information about document 2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 15\u001b[0m similar_docs \u001b[38;5;241m=\u001b[39m get_similar_documents(query, \u001b[43mdocuments\u001b[49m, embeddings)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m similar_docs:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index('faiss_index.bin')\n",
    "\n",
    "# Load the text files\n",
    "text_files = [os.path.join(text_file_folder, file_name) for file_name in file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noelm\\AppData\\Local\\Temp\\ipykernel_9336\\2437164412.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model_name=local_model, temperature=0.2, max_tokens=1024, top_p=0.95, repetition_penalty=1.2)\n"
     ]
    }
   ],
   "source": [
    "local_model = 'mistral'\n",
    "llm = ChatOllama(model_name=local_model, temperature=0.2, max_tokens=1024, top_p=0.95, repetition_penalty=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 results for the query 'What are the objectives of Budget 2024?':\n",
      "File: 15092024_142.txt, Distance: 1.7089343070983887\n",
      "File: 471 (TO).txt, Distance: 1.7089343070983887\n",
      "File: AP Ramjan.txt, Distance: 1.7089343070983887\n",
      "File: 12-Rabiul-Awal-2024.txt, Distance: 1.7089343070983887\n",
      "File: Extension-of-Ahdoc-Employees.txt, Distance: 1.7089343070983887\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the objectives of Budget 2024?\"\n",
    "query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "\n",
    "# Search the FAISS index for the nearest neighbors to the query\n",
    "k = 5  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Top {k} results for the query '{query}':\")\n",
    "for i in range(k):\n",
    "    print(f\"File: {file_names[indices[0][i]]}, Distance: {distances[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'genai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InternalServerError\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m\n\u001b[0;32m      5\u001b[0m load_dotenv()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Get the GOOGLE_API_KEY from the .env file\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'genai'"
     ]
    }
   ],
   "source": [
    "from google.api_core.exceptions import InternalServerError\n",
    "from dotenv import load_dotenv\n",
    "import genai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get the GOOGLE_API_KEY from the .env file\n",
    "API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Configure genai with the API key\n",
    "genai.configure(api_key=API_KEY)\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(query,pdfd):\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=arj_API,\n",
    "\n",
    "                                   temperature=0.3)\n",
    "\n",
    "    prompt_template = \"\"\"\" Provide the extract all the text and image text inside the content \n",
    "      Context: \\n {context}?\\n\n",
    "\n",
    "      Question: \\n {question} \\n\n",
    "\n",
    "      Answer:\n",
    "\n",
    "     \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "\n",
    "    )\n",
    "    \n",
    "    data= texto(pdfd)\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-pro\")\n",
    "    responses = model.generate_content(prompt.format(context=data, question=(query)))\n",
    "    print(responses.text)\n",
    "    return responses.text\n",
    "input= input(\"Enter the Query  : \")\n",
    "query = input\n",
    "get_output(query,\"\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
